{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMI565: Bioinformatics Programming & Scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (C) Michael Mooney (mooneymi@ohsu.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3: XML, HTML and Web Scraping\n",
    "\n",
    "** * Thanks to Ryan Swan for the materials on HTML and web scraping.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. XML Overview\n",
    "    - XML Format\n",
    "2. The Python ElementTree Class\n",
    "    - Reading XML\n",
    "    - Writing XML\n",
    "3. XML and Bioinformatics\n",
    "1. HTML\n",
    "    * Organization of HTML files\n",
    "2. LXML Package\n",
    "    * HTML as a tree structure\n",
    "    * XPath queries\n",
    "    * Element objects\n",
    "    * HTML tag attributes\n",
    "3. Beautiful Soup\n",
    "    * Soup objects and methods\n",
    "    * Using tag attributes with BeautifulSoup\n",
    "4. The Web Developers Console\n",
    "5. A note about APIs and `robots.txt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "- Python 2.7\n",
    "- `xml.etree.ElementTree` module\n",
    "- `lxml` module\n",
    "- `urllib` module\n",
    "- `BeautifulSoup (beautifulsoup4)` module\n",
    "- `io` module\n",
    "- Data Files\n",
    "    - `./data/book.xml`\n",
    "    - `./data/SHH.xml`\n",
    "- Miscellaneous Files\n",
    "    - `./images/book_tree.jpg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Overview\n",
    "\n",
    "<b>XML</b> stands for E<u>x</u>tensible <u>M</u>arkup <u>L</u>anguage, and is a set of rules for encoding documents in a machine-readable format. In bioinformatics, XML is a commonly used format for sharing heterogenous data (as opposed to delimited files, where every record (row) contains the same data elements).\n",
    "\n",
    "The World Wide Web Consortium (W3C) oversaw XML development in 1996.\n",
    "\n",
    "#### XML Design Goals:\n",
    "1. XML shall be straightforwardly usable over the Internet\n",
    "2. XML shall support a wide variety of applications\n",
    "3. XML shall be compatible with Standard Generalized Markup Language (SGML)\n",
    "4. It shall be easy to write programs that process XML documents\n",
    "5. The number of optional features in XML is to be kept to the absolute minimum\n",
    "6. XML documents should be human-legible an reasonably clear\n",
    "7. The XML design should be prepared quickly\n",
    "8. The design of XML shall be formal and concise\n",
    "9. XML documents shall be easy to create\n",
    "10. Terseness in XML markup is of minimal importance\n",
    "\n",
    "#### Why can't we use CSV formats?\n",
    "1. We usually can, but...\n",
    "1. CSV files are not always human readable (other documentation is often necessary to identify data elements)\n",
    "2. Inconsistencies are more likely \n",
    "3. CSV files don't easily support multiple levels of data\n",
    "4. CSV files don't easily support addition details such as formatting or meta data (experimental protocols, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UniProt Example: Sonic Hedgehog Protein\n",
    "\n",
    "[http://www.uniprot.org/uniprot/Q15465.xml](http://www.uniprot.org/uniprot/Q15465.xml)\n",
    "\n",
    "I've provided this file in the course materials, saved as `SHH.xml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Format\n",
    "\n",
    "The first couple lines of an XML document contain information about the XML version used, the document structure and comments:\n",
    "\n",
    "#### Version\n",
    "    <?xml version='1.0' encoding='UTF-8'?>\n",
    "    \n",
    "#### Document Type Declaration\n",
    "    <uniprot xmlns=\"http://uniprot.org/uniprot\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://uniprot.org/uniprot http://www.uniprot.org/support/docs/uniprot.xsd\">\n",
    "\n",
    "#### XML Document Body\n",
    "\n",
    "The body of an XML document contains labeled data elements. Data elements can be nested to show relationships. Data labels are called \"tags\", which can also contain attributes (values are always strings) that provide additional information about the data.\n",
    "    \n",
    "    <parent_tag>\n",
    "        <child_tag attribute1=\"value1\" attrubute2=\"value2\">data</child_tag>\n",
    "    </parent_tag>\n",
    "\n",
    "It is subjective whether to provide additional information as attributes or additional date elements:\n",
    "\n",
    "    <contact birthdate=\"1-1-1980\">\n",
    "        <name>John Smith</name>\n",
    "    </contact>\n",
    "    \n",
    "    <contact>\n",
    "        <name>John Smith</name>\n",
    "        <birthdate>1-1-1980</birthdate>\n",
    "    </contact>\n",
    "\n",
    "#### DTD and XML Schema\n",
    "\n",
    "- Document Type Definitions (DTD) and XML Schemas are two ways of describing the structure and content of an XML document\n",
    "- XML Schemas (a.k.a. XML Schema Definitions or XSDs) were designed to improve upon the shortcomings of DTDs\n",
    "    - data type support\n",
    "    - namespace aware\n",
    "- Example: the UniProt XSD - [http://www.uniprot.org/support/docs/uniprot.xsd](http://www.uniprot.org/support/docs/uniprot.xsd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElementTree\n",
    "### Reading XML\n",
    "\n",
    "There are two strategies for reading an XML document:\n",
    "\n",
    "1. Document Object Model\n",
    "    - Read the entire file, analyze relationships between elements, and build a tree structure which can be navigated/searched\n",
    "    - Uses the innate organization of the data\n",
    "    - Examples: `minidom`, `elementtree`, `lxml` Python modules\n",
    "2. Event Driven Parsers (SAX or Simple API for XML)\n",
    "    - Read the XML file and report events, such as the start and end of an element\n",
    "    - Uses less memory, no tree construction\n",
    "    - Examples: `sax` and `elementtree` Python modules\n",
    "    \n",
    "We will be covering both the `elementtree` and `lxml` modules in this lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Simple Example\n",
    "\n",
    "    <book>\n",
    "        <title>Nineteen Eighty‐Four</title>\n",
    "        <author>George Orwell</author>\n",
    "        <character>Winston Smith</character>\n",
    "        <character>Julia</character>\n",
    "    </book>\n",
    "\n",
    "    import xml.etree.ElementTree as et\n",
    "    tree = et.parse(\"1984.xml\")\n",
    "\n",
    "In the example above, `tree` is an ElementTree object containing a tree of the entire XML file. ElementTree objects are iterable objects. We can iterate through these object to access individual elements. Start by accessing the root of the tree. Each element object contains three main attributes: the tag name `tag`, the text inside the tag `text`, and the tag attributes `attrib`.\n",
    "\n",
    "    root_element = tree.getroot()\n",
    "    for element in root_element:\n",
    "        print element.tag\n",
    "        print element.text\n",
    "        print element.attrib\n",
    "\n",
    "<img src=\"./images/book_tree.jpg\" align=\"left\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another Example: `book.xml`\n",
    "\n",
    "    <book>\n",
    "\t<title>Ender's Game</title>\n",
    "\t<author>Orson Scott Card</author>\n",
    "\t<chapter>Third</chapter>\n",
    "\t<chapter>Peter</chapter>\n",
    "\t<chapter>Graff</chapter>\n",
    "    <publication_info>\n",
    "\t\t<publisher location=\"New York\">Tor Books</publisher>\n",
    "\t\t<publication_date>1985</publication_date>\n",
    "\t</publication_info>\n",
    "    </book>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'book' at 0x106476dd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.ElementTree as et\n",
    "tree = et.parse('./data/book.xml')\n",
    "root_element = tree.getroot()\n",
    "root_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element 'title' at 0x106476e10>,\n",
       " <Element 'author' at 0x106476e50>,\n",
       " <Element 'chapter' at 0x106476e90>,\n",
       " <Element 'chapter' at 0x106476ed0>,\n",
       " <Element 'chapter' at 0x106476f10>,\n",
       " <Element 'publication_info' at 0x106476f50>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(root_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(root_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Ender's Game\n",
      "author: Orson Scott Card\n",
      "chapter: Third\n",
      "chapter: Peter\n",
      "chapter: Graff\n",
      "publication_info: \n"
     ]
    }
   ],
   "source": [
    "for element in root_element:\n",
    "    print element.tag + \":\", element.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'publication_info' at 0x106476f50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_element[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(root_element[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element 'publisher' at 0x106476f90>,\n",
       " <Element 'publication_date' at 0x106476fd0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(root_element[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Ender's Game ,  {}\n",
      "author: Orson Scott Card ,  {}\n",
      "chapter: Third ,  {}\n",
      "chapter: Peter ,  {}\n",
      "chapter: Graff ,  {}\n",
      "publication_info:  ,  {}\n",
      "\tpublisher: Tor Books ,  {'location': 'New York'}\n",
      "\tpublication_date: 1985 ,  {}\n"
     ]
    }
   ],
   "source": [
    "## Each element is iterable, which allows access\n",
    "## to child elements. Here we check the length of\n",
    "## each element to get the number of children\n",
    "for element in root_element:\n",
    "    if len(element) > 0:\n",
    "        print element.tag + \":\", element.text.strip(), \", \", element.attrib\n",
    "        for child in element:\n",
    "            print \"\\t\" + child.tag + \":\", child.text.strip(), \", \", child.attrib\n",
    "    else:\n",
    "        print element.tag + \":\", element.text.strip(), \", \", element.attrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElementTree Element Methods\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr><td style=\"text-align:center\"><b>Method</b></td><td><b>Description</b></td></tr>\n",
    "<tr><td style=\"text-align:center\">`Element.iter(tag=None)`</td><td>Creates a tree iterator with the current element as root.<br />If `tag` is specified, only those elements with a tag equal to `tag` are returned by the iterator.</td></tr>\n",
    "<tr><td style=\"text-align:center\">`Element.find(tag)`</td><td>Returns the first subelement with a tag equal to `tag` or `None` if no match.</td></tr>\n",
    "<tr><td style=\"text-align:center\">`Element.findall(tag)`</td><td>Returns a list of all matching subelements.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Orson Scott Card'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author = root_element.find(\"author\")\n",
    "author.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Third', 'Peter', 'Graff']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapters = root_element.findall(\"chapter\")\n",
    "[c.text for c in chapters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the XML file is very large, you may want to use an iterator, rather than creating a tree of the entire file all at once. The `iterparse()` method implements an event-driven parser. It will return an iterator of (event, element) tuples, where event indicates the part of an element encountered (e.g. the start tag or end tag). By default, only end events are returned. Since, `iterparse()` still creates a tree in memory, you can use the `Element.clear()` method to save memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n",
      "title: Ender's Game\n",
      "end\n",
      "author: Orson Scott Card\n",
      "end\n",
      "chapter: Third\n",
      "end\n",
      "chapter: Peter\n",
      "end\n",
      "chapter: Graff\n",
      "end\n",
      "publisher: Tor Books\n",
      "end\n",
      "publication_date: 1985\n",
      "end\n",
      "publication_info: \n",
      "end\n",
      "book: \n"
     ]
    }
   ],
   "source": [
    "iter_et = et.iterparse('./data/book.xml')\n",
    "for event, element in iter_et:\n",
    "    print event\n",
    "    print element.tag + \":\", element.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Ender's Game\n",
      "author: Orson Scott Card\n",
      "chapter: Third\n",
      "chapter: Peter\n",
      "chapter: Graff\n",
      "publisher: Tor Books\n",
      "publication_date: 1985\n",
      "publication_info: \n"
     ]
    }
   ],
   "source": [
    "## Use clear() to clear each element after processing\n",
    "## including the root element\n",
    "iter_et = et.iterparse('./data/book.xml', events=['start', 'end'])\n",
    "event, root = iter_et.next()\n",
    "for event, element in iter_et:\n",
    "    if event == \"end\" and element.tag != root.tag:\n",
    "        print element.tag + \":\", element.text.strip()\n",
    "        element.clear()\n",
    "\n",
    "root.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML Namespaces\n",
    "\n",
    "XML namespaces are used to create uniquely named elements and attributes in an XML document. Since a single document may contain element names from multiple vocabularies, ambiguity can arise from the same element name used for different entity definitions. The namespace is appended to the front of tag names to create unique names. In the UniProt example shown above, the attribute `xmlns=\"http://uniprot.org/uniprot\"` specifies the UniProt namespace (in the document type declaration.\n",
    "\n",
    "A document's namespace can be extracted from the root element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{http://uniprot.org/uniprot}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the XML document's namespace\n",
    "import re\n",
    "shh_tree = et.parse('./data/SHH.xml')\n",
    "shh_root = shh_tree.getroot()\n",
    "namespace = re.match(r\"{.*}\", shh_root.tag).group()\n",
    "namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SHH_HUMAN'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Append the namespace to any element name\n",
    "## you want to find\n",
    "entry = shh_root.find(namespace+'entry')\n",
    "entry.find(namespace+'name').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SHH_HUMAN'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = {'uniprot':'http://uniprot.org/uniprot'}\n",
    "entry = shh_root.find('uniprot:entry', ns)\n",
    "entry.find(\"uniprot:name\", ns).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing XML\n",
    "\n",
    "#### Methods for Writing XML\n",
    "<table align=\"left\">\n",
    "<tr><td style=\"text-align:center\"><b>Method</b></td><td><b>Description</b></td></tr>\n",
    "<tr><td style=\"text-align:center\">`et.Element(tag)`</td><td>Creates an element with the specified tag. Returns an element object.</td></tr>\n",
    "<tr><td style=\"text-align:center\">`et.SubElement(element, tag)`</td><td>Creates a child element under the specified element.</td></tr>\n",
    "<tr><td style=\"text-align:center\">`Element.set(key, value)`</td><td>Sets the attributes of an element.</td></tr>\n",
    "<tr><td style=\"text-align:center\">`et.ElementTree(root)`</td><td>Returns an ElementTree object.</td></tr>\n",
    "<tr><td style=\"text-align:center\">`ElementTree.write(file)`</td><td>Writes an ElementTree object to a file.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a simple XML file\n",
    "root = et.Element(\"book\")\n",
    "title = et.SubElement(root, \"title\")\n",
    "title.text = \"Nineteen Eighty-Four\"\n",
    "author = et.SubElement(root, \"author\")\n",
    "author.text = \"George Orwell\"\n",
    "\n",
    "pub_info = et.SubElement(root, \"publication_info\")\n",
    "pub = et.SubElement(pub_info, \"publisher\")\n",
    "pub.text = \"Secker and Warburg\"\n",
    "pub.attrib = {\"location\": \"London\"}\n",
    "tree = et.ElementTree(root)\n",
    "tree.write(\"1984.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<book><title>Nineteen Eighty-Four</title><author>George Orwell</author><publication_info><publisher location=\"London\">Secker and Warburg</publisher></publication_info></book>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('1984.xml') as fh:\n",
    "    data = fh.read()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawbacks to XML?\n",
    "\n",
    "- More difficult to parse than CSV\n",
    "- Verbose syntax means larger files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML and Bioinformatics\n",
    "#### SBML (Systems Biology Markup Language)\n",
    "- Used to communicate models of biological processes (cell-signaling pathways, regulatory networks). Models can represent:\n",
    "    - Chemical Equations\n",
    "    - Cellular Components: nucleus, cytoplasm, etc.\n",
    "    - Species: genomes, proteomes, etc.\n",
    "- Supported by many applications: [http://sbml.org/SBML_Software_Guide](http://sbml.org/SBML_Software_Guide)\n",
    "- [http://www.ebi.ac.uk/biomodels-main/](http://www.ebi.ac.uk/biomodels-main/)\n",
    "\n",
    "#### KGML (KEGG Markup Language)\n",
    "- A format for KEGG pathway maps\n",
    "    - [http://www.kegg.jp/kegg/xml/](http://www.kegg.jp/kegg/xml/)\n",
    "    \n",
    "#### PDBML (Protein Databank Markup Language)\n",
    "- Describes 3D protein structure\n",
    "    - relative atomic coordinates\n",
    "    - secondary structure assignment\n",
    "    - atomic connectivity\n",
    "- [http://www.rcsb.org/pdb/home/home.do](http://www.rcsb.org/pdb/home/home.do)\n",
    "- [http://pdbml.pdb.org/](http://pdbml.pdb.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "Hypertext Markup Language (HTML) is the basis for most pages that are served on the internet. HTML is actually very similar to XML (Extensible Markup Language), with the caveat that it also contains presentation semantics, which are attributes that specify how information is meant to be displayed or arranged on a screen. But overall, the nested format is almost exactly like an XML document, and because of that, we can extract information from a standard HTML page exactly the same way we would from an XML document. Below is a simple example of an HTML document:\n",
    "\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Hey look, a webpage!</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p>webpage goes here</p>\n",
    "    </body>\n",
    "    </html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LXML package\n",
    "\n",
    "The LXML package for Python contains methods to read HTML pages like a tree structure. It uses a querying syntax called XML Path Language (XPath) to parse the tree structure and return relevent information from the document.\n",
    "\n",
    "Before we get started, it helps to have an idea of some of the ways that HTML arranges documents. Most scrapable HTML data is contained in tables like the one at http://www.bioinformatics.org/sms/iupac.html. HTML tables are arranged in the following format:\n",
    "\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td></td>\n",
    "            <td></td>\n",
    "            <td></td>\n",
    "            ...\n",
    "        </tr>\n",
    "        <tr>\n",
    "            ...\n",
    "        </tr>\n",
    "    </table>\n",
    "\n",
    "This general format specifies table rows and table dividers, where each divider is a different column. The data in the table is contained inside each of the nested <td></td> tag pairs. \n",
    "\n",
    "XPath querying allows us to find specific kinds of elements and their contents. Let's use the tables on the following webpage as an example: [http://www.bioinformatics.org/sms/iupac.html](http://www.bioinformatics.org/sms/iupac.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from urllib import urlopen # lets us open files from web addresses\n",
    "from io import StringIO # This will help us deal with string inputs\n",
    "\n",
    "## Get the code from the url\n",
    "html = urlopen(\"http://www.bioinformatics.org/sms/iupac.html\").read()\n",
    "\n",
    "## First we have some housekeeping. StringIO wants to see a unicode string, \n",
    "## so we have to change the encoding on our html so it can be read.\n",
    "\n",
    "html = html.decode('utf-8')\n",
    "\n",
    "## Next we have to create a parser that will read the info from the HTML \n",
    "## file and tell it what kind of data it will be receiving\n",
    "\n",
    "parser = etree.HTMLParser()\n",
    "tree = etree.parse(StringIO(html),parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the webpage represented as a tree of data. This tree is an iterable object, just like we saw above when working with XML documents. We can do all sorts of things now.\n",
    "\n",
    "For example with can iterate through the tree with a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element head at 0x1069b7ea8>\n",
      "\t<Element meta at 0x109315098>\n",
      "\t<Element meta at 0x109315050>\n",
      "\t<Element meta at 0x1093151b8>\n",
      "\t<Element title at 0x109315170>\n",
      "<Element body at 0x1064e7758>\n",
      "\t<Element table at 0x1069b7ea8>\n",
      "\t<Element br at 0x1093151b8>\n",
      "\t<Element table at 0x109315098>\n"
     ]
    }
   ],
   "source": [
    "## Note: here we are only showing two levels of the tree\n",
    "root = tree.getroot()\n",
    "\n",
    "for e in root:\n",
    "    print e\n",
    "    for i in e:\n",
    "        print '\\t' + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The following function will print the entire tree structure\n",
    "## This function looks in each element node, and if it has \n",
    "## contents it performs the same action on the descendent node\n",
    "## Note that this is an example of recursion - a function \n",
    "## that calls itself.\n",
    "\n",
    "def parseTree(e,t='\\t'):\n",
    "    for i in e:\n",
    "        print str(t) + str(i)\n",
    "        parseTree(i,t=t + '\\t')\n",
    "\n",
    "parseTree(tree.getroot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `etree` object has a method called `xpath()`, which allows us to perform queries on the tree structure to identify specific elements within the HTML document. For example, if we want to find all tables within the body of the document we would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element table at 0x109315248>, <Element table at 0x109315098>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This will return a list of table elements\n",
    "tables = tree.xpath('body/table')\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use tag attributes to perform more specific queries. For instance, we know that the table containing amino acid codes has three columns. To extract this table we could do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element table at 0x109315098>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This will find all tables with three columns\n",
    "## Note: the // means it will look anywhere under the current element (root in this case) \n",
    "## (i.e. the table could be nested within another element)\n",
    "amino = tree.xpath(\"\"\"//table[@cols='3']\"\"\")\n",
    "amino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "A\n",
      "Ala\n",
      "Alanine\n",
      "C\n",
      "Cys\n",
      "Cysteine\n",
      "D\n",
      "Asp\n",
      "Aspartic Acid\n",
      "E\n",
      "Glu\n",
      "Glutamic Acid\n",
      "F\n",
      "Phe\n",
      "Phenylalanine\n",
      "G\n",
      "Gly\n",
      "Glycine\n",
      "H\n",
      "His\n",
      "Histidine\n",
      "I\n",
      "Ile\n",
      "Isoleucine\n",
      "K\n",
      "Lys\n",
      "Lysine\n",
      "L\n",
      "Leu\n",
      "Leucine\n",
      "M\n",
      "Met\n",
      "Methionine\n",
      "N\n",
      "Asn\n",
      "Asparagine\n",
      "P\n",
      "Pro\n",
      "Proline\n",
      "Q\n",
      "Gln\n",
      "Glutamine\n",
      "R\n",
      "Arg\n",
      "Arginine\n",
      "S\n",
      "Ser\n",
      "Serine\n",
      "T\n",
      "Thr\n",
      "Threonine\n",
      "V\n",
      "Val\n",
      "Valine\n",
      "W\n",
      "Trp\n",
      "Tryptophan\n",
      "Y\n",
      "Tyr\n",
      "Tyrosine\n"
     ]
    }
   ],
   "source": [
    "## We can iterate through this table to get the data\n",
    "for row in amino[0]:\n",
    "    for cell in row:\n",
    "        print cell.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the column headers are missing above. This is because that text is not directly within the table cells, it is actually nested within a `<font>` tag, which allows additional formatting of the text. The code below will solve this problem. The Xpath `text()` function will extract text, and using the `//` means that it will find text anywere under the `<td>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IUPAC amino acid code\n",
      "Three letter code\n",
      "Amino acid\n",
      "A\n",
      "Ala\n",
      "Alanine\n",
      "C\n",
      "Cys\n",
      "Cysteine\n",
      "D\n",
      "Asp\n",
      "Aspartic Acid\n",
      "E\n",
      "Glu\n",
      "Glutamic Acid\n",
      "F\n",
      "Phe\n",
      "Phenylalanine\n",
      "G\n",
      "Gly\n",
      "Glycine\n",
      "H\n",
      "His\n",
      "Histidine\n",
      "I\n",
      "Ile\n",
      "Isoleucine\n",
      "K\n",
      "Lys\n",
      "Lysine\n",
      "L\n",
      "Leu\n",
      "Leucine\n",
      "M\n",
      "Met\n",
      "Methionine\n",
      "N\n",
      "Asn\n",
      "Asparagine\n",
      "P\n",
      "Pro\n",
      "Proline\n",
      "Q\n",
      "Gln\n",
      "Glutamine\n",
      "R\n",
      "Arg\n",
      "Arginine\n",
      "S\n",
      "Ser\n",
      "Serine\n",
      "T\n",
      "Thr\n",
      "Threonine\n",
      "V\n",
      "Val\n",
      "Valine\n",
      "W\n",
      "Trp\n",
      "Tryptophan\n",
      "Y\n",
      "Tyr\n",
      "Tyrosine\n"
     ]
    }
   ],
   "source": [
    "for i in tree.xpath(\"\"\"//table[@cols='3']/tr/td//text()\"\"\"):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start using for loops to write more interesting queries, and convert the entire table to a data structure  we can more easily use.\n",
    "\n",
    "One thing to keep in mind is that once you have focused on a particular part of the tree, your position is defined relative to that element. However, the object still contains the full information about the whole HTML document's tree. You are able to start a query with the absolute path of the full tree with `/` or you are able to use `.` in order to define a query relative to your current position. Here we use the `.` operator to define a path relative to the current element (e.g. the table element stored in `amino[0]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['IUPAC amino acid code', 'Three letter code', 'Amino acid'],\n",
       " ['A', 'Ala', 'Alanine'],\n",
       " ['C', 'Cys', 'Cysteine'],\n",
       " ['D', 'Asp', 'Aspartic Acid'],\n",
       " ['E', 'Glu', 'Glutamic Acid'],\n",
       " ['F', 'Phe', 'Phenylalanine'],\n",
       " ['G', 'Gly', 'Glycine'],\n",
       " ['H', 'His', 'Histidine'],\n",
       " ['I', 'Ile', 'Isoleucine'],\n",
       " ['K', 'Lys', 'Lysine'],\n",
       " ['L', 'Leu', 'Leucine'],\n",
       " ['M', 'Met', 'Methionine'],\n",
       " ['N', 'Asn', 'Asparagine'],\n",
       " ['P', 'Pro', 'Proline'],\n",
       " ['Q', 'Gln', 'Glutamine'],\n",
       " ['R', 'Arg', 'Arginine'],\n",
       " ['S', 'Ser', 'Serine'],\n",
       " ['T', 'Thr', 'Threonine'],\n",
       " ['V', 'Val', 'Valine'],\n",
       " ['W', 'Trp', 'Tryptophan'],\n",
       " ['Y', 'Tyr', 'Tyrosine']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remember here we are only interested in the amino acid table\n",
    "## Use the . to ensure you are searching for rows within that table only\n",
    "table_list = []\n",
    "for tr in amino[0].xpath('./tr'):\n",
    "    table_list.append(tr.xpath('./td//text()'))\n",
    "table_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup \n",
    "\n",
    "While that was certainly a fun demonstration of how HTML is organized and can be digested for further analysis, manual XPath evaluations can be a tedious process. Beautiful Soup is a package meant to make the process of getting information from web documents much simpler.\n",
    "\n",
    "In Beautiful Soup, we first import the package in order to create a \"soup\" object. Here we use the html object that we acquired earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "soup = bs(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can perform all sorts of different manipulations on the data, and Beautiful Soup takes care of the many of the details behind the scenes. Let's just take a look a couple quick examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<table border=\"\" cellpadding=\"2\" cellspacing=\"0\" cols=\"2\" width=\"350\">\\n<tr>\\n<td bgcolor=\"#B0C4DE\"><font color=\"#000000\">IUPAC nucleotide code</font></td>\\n<td bgcolor=\"#B0C4DE\"><font color=\"#000000\">Base</font></td>\\n</tr>\\n<tr>\\n<td>A</td>\\n<td>Adenine</td>\\n</tr>\\n<tr>\\n<td>C</td>\\n<td>Cytosine</td>\\n</tr>\\n<tr>\\n<td>G</td>\\n<td>Guanine</td>\\n</tr>\\n<tr>\\n<td>T (or U)</td>\\n<td>Thymine (or Uracil)</td>\\n</tr>\\n<tr>\\n<td>R</td>\\n<td>A or G</td>\\n</tr>\\n<tr>\\n<td>Y</td>\\n<td>C or T</td>\\n</tr>\\n<tr>\\n<td>S</td>\\n<td>G or C</td>\\n</tr>\\n<tr>\\n<td>W</td>\\n<td>A or T</td>\\n</tr>\\n<tr>\\n<td>K</td>\\n<td>G or T</td>\\n</tr>\\n<tr>\\n<td>M</td>\\n<td>A or C</td>\\n</tr>\\n<tr>\\n<td>B</td>\\n<td>C or G or T</td>\\n</tr>\\n<tr>\\n<td>D</td>\\n<td>A or G or T</td>\\n</tr>\\n<tr>\\n<td>H</td>\\n<td>A or C or T</td>\\n</tr>\\n<tr>\\n<td>V</td>\\n<td>A or C or G</td>\\n</tr>\\n<tr>\\n<td>N</td>\\n<td>any base</td>\\n</tr>\\n<tr>\\n<td>. or -</td>\\n<td>gap</td>\\n</tr>\\n</table>,\n",
       " <table border=\"\" cellpadding=\"2\" cellspacing=\"0\" cols=\"3\" width=\"350\">\\n<tr>\\n<td bgcolor=\"#B0C4DE\"><font color=\"#000000\">IUPAC amino acid code</font></td>\\n<td bgcolor=\"#B0C4DE\"><font color=\"#000000\">Three letter code</font></td>\\n<td bgcolor=\"#B0C4DE\"><font color=\"#000000\">Amino acid</font></td>\\n</tr>\\n<tr>\\n<td>A</td>\\n<td>Ala</td>\\n<td>Alanine</td>\\n</tr>\\n<tr>\\n<td>C</td>\\n<td>Cys</td>\\n<td>Cysteine</td>\\n</tr>\\n<tr>\\n<td>D</td>\\n<td>Asp</td>\\n<td>Aspartic Acid</td>\\n</tr>\\n<tr>\\n<td>E</td>\\n<td>Glu</td>\\n<td>Glutamic Acid</td>\\n</tr>\\n<tr>\\n<td>F</td>\\n<td>Phe</td>\\n<td>Phenylalanine</td>\\n</tr>\\n<tr>\\n<td>G</td>\\n<td>Gly</td>\\n<td>Glycine</td>\\n</tr>\\n<tr>\\n<td>H</td>\\n<td>His</td>\\n<td>Histidine</td>\\n</tr>\\n<tr>\\n<td>I</td>\\n<td>Ile</td>\\n<td>Isoleucine</td>\\n</tr>\\n<tr>\\n<td>K</td>\\n<td>Lys</td>\\n<td>Lysine</td>\\n</tr>\\n<tr>\\n<td>L</td>\\n<td>Leu</td>\\n<td>Leucine</td>\\n</tr>\\n<tr>\\n<td>M</td>\\n<td>Met</td>\\n<td>Methionine</td>\\n</tr>\\n<tr>\\n<td>N</td>\\n<td>Asn</td>\\n<td>Asparagine</td>\\n</tr>\\n<tr>\\n<td>P</td>\\n<td>Pro</td>\\n<td>Proline</td>\\n</tr>\\n<tr>\\n<td>Q</td>\\n<td>Gln</td>\\n<td>Glutamine</td>\\n</tr>\\n<tr>\\n<td>R</td>\\n<td>Arg</td>\\n<td>Arginine</td>\\n</tr>\\n<tr>\\n<td>S</td>\\n<td>Ser</td>\\n<td>Serine</td>\\n</tr>\\n<tr>\\n<td>T</td>\\n<td>Thr</td>\\n<td>Threonine</td>\\n</tr>\\n<tr>\\n<td>V</td>\\n<td>Val</td>\\n<td>Valine</td>\\n</tr>\\n<tr>\\n<td>W</td>\\n<td>Trp</td>\\n<td>Tryptophan</td>\\n</tr>\\n<tr>\\n<td>Y</td>\\n<td>Tyr</td>\\n<td>Tyrosine</td>\\n</tr>\\n</table>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find all tables in the document\n",
    "tables = soup.find_all(\"table\")\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table border=\"\" cellpadding=\"2\" cellspacing=\"0\" cols=\"3\" width=\"350\">\\n<tr>\\n<td bgcolor=\"#B0C4DE\"><font color=\"#000000\">IUPAC amino acid code</font></td>\\n<td bgcolor=\"#B0C4DE\"><font color=\"#000000\">Three letter code</font></td>\\n<td bgcolor=\"#B0C4DE\"><font color=\"#000000\">Amino acid</font></td>\\n</tr>\\n<tr>\\n<td>A</td>\\n<td>Ala</td>\\n<td>Alanine</td>\\n</tr>\\n<tr>\\n<td>C</td>\\n<td>Cys</td>\\n<td>Cysteine</td>\\n</tr>\\n<tr>\\n<td>D</td>\\n<td>Asp</td>\\n<td>Aspartic Acid</td>\\n</tr>\\n<tr>\\n<td>E</td>\\n<td>Glu</td>\\n<td>Glutamic Acid</td>\\n</tr>\\n<tr>\\n<td>F</td>\\n<td>Phe</td>\\n<td>Phenylalanine</td>\\n</tr>\\n<tr>\\n<td>G</td>\\n<td>Gly</td>\\n<td>Glycine</td>\\n</tr>\\n<tr>\\n<td>H</td>\\n<td>His</td>\\n<td>Histidine</td>\\n</tr>\\n<tr>\\n<td>I</td>\\n<td>Ile</td>\\n<td>Isoleucine</td>\\n</tr>\\n<tr>\\n<td>K</td>\\n<td>Lys</td>\\n<td>Lysine</td>\\n</tr>\\n<tr>\\n<td>L</td>\\n<td>Leu</td>\\n<td>Leucine</td>\\n</tr>\\n<tr>\\n<td>M</td>\\n<td>Met</td>\\n<td>Methionine</td>\\n</tr>\\n<tr>\\n<td>N</td>\\n<td>Asn</td>\\n<td>Asparagine</td>\\n</tr>\\n<tr>\\n<td>P</td>\\n<td>Pro</td>\\n<td>Proline</td>\\n</tr>\\n<tr>\\n<td>Q</td>\\n<td>Gln</td>\\n<td>Glutamine</td>\\n</tr>\\n<tr>\\n<td>R</td>\\n<td>Arg</td>\\n<td>Arginine</td>\\n</tr>\\n<tr>\\n<td>S</td>\\n<td>Ser</td>\\n<td>Serine</td>\\n</tr>\\n<tr>\\n<td>T</td>\\n<td>Thr</td>\\n<td>Threonine</td>\\n</tr>\\n<tr>\\n<td>V</td>\\n<td>Val</td>\\n<td>Valine</td>\\n</tr>\\n<tr>\\n<td>W</td>\\n<td>Trp</td>\\n<td>Tryptophan</td>\\n</tr>\\n<tr>\\n<td>Y</td>\\n<td>Tyr</td>\\n<td>Tyrosine</td>\\n</tr>\\n</table>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find the first table that matches some criteria\n",
    "table = soup.find(\"table\",{\"width\":\"350\",\"cols\":\"3\"})\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'IUPAC amino acid code', u'Three letter code', u'Amino acid'],\n",
       " [u'A', u'Ala', u'Alanine'],\n",
       " [u'C', u'Cys', u'Cysteine'],\n",
       " [u'D', u'Asp', u'Aspartic Acid'],\n",
       " [u'E', u'Glu', u'Glutamic Acid'],\n",
       " [u'F', u'Phe', u'Phenylalanine'],\n",
       " [u'G', u'Gly', u'Glycine'],\n",
       " [u'H', u'His', u'Histidine'],\n",
       " [u'I', u'Ile', u'Isoleucine'],\n",
       " [u'K', u'Lys', u'Lysine'],\n",
       " [u'L', u'Leu', u'Leucine'],\n",
       " [u'M', u'Met', u'Methionine'],\n",
       " [u'N', u'Asn', u'Asparagine'],\n",
       " [u'P', u'Pro', u'Proline'],\n",
       " [u'Q', u'Gln', u'Glutamine'],\n",
       " [u'R', u'Arg', u'Arginine'],\n",
       " [u'S', u'Ser', u'Serine'],\n",
       " [u'T', u'Thr', u'Threonine'],\n",
       " [u'V', u'Val', u'Valine'],\n",
       " [u'W', u'Trp', u'Tryptophan'],\n",
       " [u'Y', u'Tyr', u'Tyrosine']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Iterate through the table and create a list of lists\n",
    "table_list2 = []\n",
    "for row in table.findAll(\"tr\"):\n",
    "    cells = row.findAll(\"td\")\n",
    "    newCells = []\n",
    "    for c in cells:\n",
    "        newCells.append(c.get_text())\n",
    "    table_list2.append(newCells)\n",
    "table_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Developer's Console\n",
    "\n",
    "Both Chrome and Firefox are equipped with a developer's console, meant for debugging code while writing websites. This console can also be used to see what elements your computer is interfacing with while you surf the web. \n",
    "\n",
    "To open the developer's console in firefox, press Ctrl+Shift+K in Windows or Cmd+Opt+K in OSX. The network tab will allow you to see what information is being sent when, while the Inspector tab allows you to hover over code and see what element of the page it represents. \n",
    "\n",
    "Chrome's developer console can be accessed with Ctrl+Shift+J on Windows or Cmd+Opt+J on OSX. While the tabs are named slightly differently, the functions are essentially the same. Notably, Chrome provides native support for web scraping, though the data it gives are usually oriented more toward the organization of entire sites and less toward acquiring data from an individual page.\n",
    "\n",
    "If you plan on getting data from the web, this is an invaluable tool that will save you a lot of time finding out where data is stored.\n",
    "\n",
    "## A Word On APIs And robots.txt\n",
    "\n",
    "Before scraping a site, it is worth taking a couple of things into account in order to make sure that you are a good citizen of the web.  The robots.txt file located in the root directory of most websites will usually give you an idea of which directories are and are not allowed for web scraping. It is good practice if you are scraping a large amount of data to make sure that you adhere to the areas that are described by robots.txt with the \"Allow:\" tag. \n",
    "\n",
    "Many sites also provide an Application Programming Interface (API) that allows you to acquire information directly without scraping web data from the HTML interface, saving both you and the site manager time and money. If an API is available, it is almost always advisable to make use of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Class Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Exercise 1.\n",
    "## Extract the title and author list for the \n",
    "## first reference in SHH.xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Exercise 2.\n",
    "## Using either lxml or BeautifulSoup, scrape the values from the first \n",
    "## table which contains nucleotides and their corresponding name\n",
    "## Create a dictionary from these values where the nucleotide code is the key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <u>Python Essential Reference</u>, David Beazley, 4th Edition, Addison‐Wesley (2008)\n",
    "- <u>Python for Bioinformatics</u>, Sebastian Bassi, CRC Press (2010)\n",
    "- [http://en.wikipedia.org/wiki/XML](http://en.wikipedia.org/wiki/XML)\n",
    "- [http://docs.python.org/](http://docs.python.org/)\n",
    "- [https://docs.python.org/2/library/xml.etree.elementtree.html](https://docs.python.org/2/library/xml.etree.elementtree.html)\n",
    "- [LXML HTML Xpath Tutorial](http://lxml.de/parsing.html)\n",
    "- [BeautifulSoup Documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [XPath Syntax Guide](https://www.w3schools.com/xml/xpath_syntax.asp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Updated: 10-Sep-2017"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
